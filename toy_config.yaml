# Model configuration
vocab_size: 100
d_model: 128
n_heads: 4
n_layers: 2

policy_init_logit_value: 0.0  # Initial logit value for the categorical policy

device: cuda

ref_pol_eps: 1e-10 
reward_type: "linear"  # "linear", "zero-one"

# Optional pretrain policy to match reference
pretrain_policy:
  enable: False
  steps: 500
  lr: 1e-3
  log_every: 10
  

# Training configuration
num_iterations: 501
batch_size: 32
learning_rate: 2e-4
grad_clip: 1.0

kl_coeff: 0.1
entropy_coeff: 0.1

compute_kl_to_target: False

seed: 0

loss_cfg:
  advantage_type: "baseline_mean"  # "raw", "baseline_mean", "baseline_mean_std"
  kl_grad_type: "lowvar"  # "lowvar", "vanilla", "mse"
  entropy_grad_type: "analytical"  # "analytical", "vanilla", "mse"

# Logging configuration
# log_every: 10
save_every: 50
save_policy_marginal: True  # Optionally save marginalized policy distribution in checkpoints


# ==
# Hydra runner and configs
# Config searchpath follows example from: https://github.com/facebookresearch/hydra/tree/main/examples/advanced/config_search_path
hydra:
  run:
    dir: /gpfs/data/ranganathlab/Jatin/diversity/results/${now:%Y.%m.%d}/test_${now:%H%M%S}
  sweep:
    dir: /gpfs/data/ranganathlab/Jatin/diversity/results/${now:%Y.%m.%d}/${now:%H%M%S}
    subdir: ${hydra.job.num}
  job:
    chdir: True  # let hydra change runtime cwd (default False for >=1.2)
